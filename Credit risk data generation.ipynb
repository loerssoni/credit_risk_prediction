{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA MERGING AND VALIDATION FOR CREDIT RISK ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by importing relevant libraries, importing and joining the datasets and exploring some missingness of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans = pd.read_csv('trans.asc',sep=';')\n",
    "client = pd.read_csv('client.asc',sep=';')\n",
    "account = pd.read_csv('account.asc',sep=';')\n",
    "disp = pd.read_csv('disp.asc',sep=';')\n",
    "order = pd.read_csv('order.asc',sep=';')\n",
    "loan = pd.read_csv('loan.asc',sep=';')\n",
    "card = pd.read_csv('card.asc',sep=';')\n",
    "district = pd.read_csv('district.asc',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and merging the loan and account -related datasets\n",
    "We merge the static datasets and explore proportional missingness in the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(loan, account,on='account_id', suffixes=['_loan','_acnt'], how='outer')\n",
    "df = pd.merge(df, disp, on='account_id', how='outer')\n",
    "df = pd.merge(df, client, on='client_id', how='outer', suffixes = ['_clnt','_acnt'])\n",
    "df = pd.merge(df, district, left_on='district_id_clnt', right_on='A1', how='outer')\n",
    "df = pd.merge(df, card, on='disp_id', how='outer', suffixes=['', '_card'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(df.isna())/len(df))\n",
    "print('\\nThere are {} loans in the data.'.format(len(df.loan_id.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are relatively few loans available in the datasets, and most accounts do not have a loan associated with them.\n",
    "The missingness of credit card data is no issue, as the type and issue date of cards are likely not relevant to the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation for loans\n",
    "\n",
    "We first drop observations on users that do not have a loan associated in any of the accounts they participate in.\n",
    "\n",
    "We also drop some irrelevant columns. Most of the identification was only necessary for joining the data, so they are dropped. Most of the demographic data describes essentially the population and urbanization in the area, so this redundant information is dropped.\n",
    "There are only 5 junior and 3 gold cards in the data so the card type is dropped as well.\n",
    "\n",
    "We then encode features into formats suitable for machine learning.\n",
    "WE CREATE\n",
    "\n",
    "The demographic data is available only as static values measured after some of the loans in the data are already issued. Because ex-ante values are not available, we make the assumption that  the demographics do not drastically change across years. However, if the demographics turn out to be important in predicting credit defaults, this problem should be readdressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = df[~pd.isna(df.loan_id)]\n",
    "\n",
    "loans.drop(['account_id','district_id_acnt', 'A1', 'A2', 'A3',\n",
    "            'A5','A6', 'A7', 'A8', 'A9','district_id_clnt', 'disp_id',\n",
    "            'client_id', 'card_id', 'type_card'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#create dummy for whether the loan completed successfully\n",
    "loans['target'] = (loans.status == 'B').astype(int) + (loans.status == 'D').astype(int)\n",
    "\n",
    "#convert date columns to datetime\n",
    "loans['date_loan'] = pd.to_datetime(loans.date_loan, format='%y%m%d')\n",
    "loans['issued'] = pd.to_datetime(loans.issued.str[:6])\n",
    "loans['date_acnt'] = pd.to_datetime(loans.date_acnt, format='%y%m%d')\n",
    "\n",
    "#find gender (encoded into the birthnumber) and convert birthdate into datetime\n",
    "loans['gender'] = (loans.birth_number % 10000 > 5000).astype(int)\n",
    "loans['birthdate'] = loans.birth_number - 5000 * loans.gender + 19000000\n",
    "loans['birthdate'] = pd.to_datetime(loans.birthdate, format='%Y%m%d')\n",
    "\n",
    "#find the age of applicant and the account at the time of loan issuance\n",
    "loans['appl_age'] = (loans.date_loan - loans.birthdate).dt.days / 365.25\n",
    "loans['accnt_age'] = (loans.date_loan - loans.date_acnt).dt.days / 365.25\n",
    "\n",
    "\n",
    "\n",
    "#create dummy for whether the account has an associated card at the time of loan issuance\n",
    "loans['issued'] = (loans.issued < loans.date_loan).astype(int)\n",
    "\n",
    "#create dummies for the frequency of statement issuance and the account type\n",
    "loans = pd.get_dummies(loans, columns=['frequency', 'type'], drop_first=True)\n",
    "\n",
    "\n",
    "# select unemployment and crime from the demographic statistics.\n",
    "loans['unempl'] = np.select([loans.date_loan.dt.year > 1996,\n",
    "                             loans.date_loan.dt.year < 1997],\n",
    "                            [loans.A13, loans.A12])\n",
    "\n",
    "loans['crime'] = np.select([loans.date_loan.dt.year > 1996, loans.date_loan.dt.year < 1997],\n",
    "          [loans.A16, loans.A15])\n",
    "\n",
    "# convert the columns to numeric values and scale the crime numbers for population\n",
    "loans['unempl'] = pd.to_numeric(loans.unempl, errors='coerce')\n",
    "loans['crime'] = pd.to_numeric(loans.crime, errors='coerce') / loans.A4\n",
    "loans['A14'] = loans.A14 / loans.A4 * 100\n",
    "\n",
    "#finally, aggregate to loan-level from client-level data\n",
    "loans = loans.groupby('loan_id').agg('mean')\n",
    "loans['multi'] = np.select([loans.type_OWNER < 1], [1], 0)\n",
    "\n",
    "#drop unnecessary columns\n",
    "loans.drop(['birth_number','type_OWNER','A13','A16'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.columns = ['amount', 'duration', 'payments', 'pop', 'urban_rat', 'avg_sal', 'rat_urban', 'card',\n",
    "       'target', 'gender', 'appl_age', 'accnt_age',\n",
    "       'freq_trans', 'freq_weekly', 'unempl',\n",
    "       'crime', 'multi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(loans.isna()))\n",
    "print('\\nThere are {} loans in the data.'.format(len(loans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and merge transaction data\n",
    "\n",
    "Note that, because the dataset is relatively small, we introduce some redundancy for a while by joining the full dataset of static information. \n",
    "This is done to spare lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pd.read_csv('trans.asc', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pd.merge(trans, df, on='account_id', suffixes=['_trans', ''], how='left')\n",
    "#Subset the data set to transactions for accounts with loans:\n",
    "trans_loans = trans[~pd.isna(trans.loan_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total transactions: ', len(trans))\n",
    "print('Transactions for accounts associated with a loan: ', len(trans_loans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime wrangling\n",
    "\n",
    "Because we're interested in predicting bad loans, we should use transaction data from only prior to giving out the loan.\n",
    "In order to work with the dates, we will first transform them to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_loans.date = pd.to_datetime(trans_loans.date, format='%y%m%d')\n",
    "trans_loans.date_loan = pd.to_datetime(trans_loans.date_loan, format='%y%m%d')\n",
    "\n",
    "#filter to transactions prior to loan issuance\n",
    "trans_loans = trans_loans[trans_loans.date < trans_loans.date_loan]\n",
    "\n",
    "print('We end up with {} ex-ante transactions for our final data.'.format(len(trans_loans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore missingness in the transaction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.sum(trans.isna())/len(trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.sum(trans_loans.isna())/len(trans_loans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bank and account columns report the bank and account of a partner in a transaction. As such, the specific bank and of a partner are likely not relevant and missingness should not be an issue.\n",
    "\n",
    "Because we want to utilize the transactions for predicting credit defaults, transaction types are intuitively important. The operation and k_symbol columns describe the transaction type. We will explore the missing operation data further by examining, whethere there are cases where both operation and k_symbol are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Percentage of cases where both operation and k_symbol are missing: ')\n",
    "print(np.sum(trans.operation.isna() & trans.k_symbol.isna())/len(trans))\n",
    "print('Percentage of cases where both operation and k_symbol are reported: ')\n",
    "print(np.sum(~trans.operation.isna() & ~trans.k_symbol.isna())/len(trans))\n",
    "\n",
    "print('Percentage of cases where both operation and k_symbol are missing: ')\n",
    "print(np.sum(trans_loans.operation.isna() & trans_loans.k_symbol.isna())/len(trans))\n",
    "print('Percentage of cases where both operation and k_symbol are reported: ')\n",
    "print(np.sum(~trans_loans.operation.isna() & ~trans_loans.k_symbol.isna())/len(trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, while operation and k_symbol are missing at times, there are no cases where neither one is present. Thus, missingness should be no issue.\n",
    "\n",
    "Because missing data is not an issue and we do not need to use the transaction data to predict missing values, we can focus on the subset of the transactions data, where there are loans present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating transactions\n",
    "\n",
    "For now, we'll drop the other info and focus on aggregating the transactions data to loan-level.\n",
    "\n",
    "First, we drop irrelevant columns and generate some dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_loans = trans_loans[['loan_id', 'date','date_loan', 'type_trans', 'operation',\n",
    "       'amount_trans', 'balance', 'k_symbol']]\n",
    "\n",
    "trans_loans = trans_loans[trans_loans.k_symbol != 'UROK']\n",
    "trans_loans = trans_loans[trans_loans.k_symbol != 'SLUZBY']\n",
    "trans_loans.k_symbol.fillna(trans_loans.operation, inplace=True)\n",
    "\n",
    "trans_loans['b_deposit'] = trans_loans.k_symbol.str.contains('PREVOD Z').astype(int)\n",
    "trans_loans['c_deposit'] = trans_loans.k_symbol.str.contains('VKLAD').astype(int)\n",
    "trans_loans['withdr'] = trans_loans.k_symbol.str.contains('VYBER').astype(int)\n",
    "trans_loans['sanc'] = trans_loans.k_symbol.str.contains('SANK').astype(int)\n",
    "trans_loans['b_withdr'] = trans_loans.operation.str.contains('PREVOD NA').astype(int)\n",
    "\n",
    "trans_loans.drop(['type_trans','operation','k_symbol'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(data, time_window_max = 100, time_window_min = 0):  \n",
    "    trans_agg = data[(data.date_loan - data.date).dt.days < time_window_max]\n",
    "    trans_agg = data[(data.date_loan - data.date).dt.days > time_window_min]\n",
    "    trans_agg['balance_start'] = trans_agg.balance - trans_agg.amount_trans\n",
    "    trans_agg['transactions'] = 1\n",
    "    trans_agg['net_cdeposit'] = trans_agg.c_deposit * trans_agg.amount_trans \\\n",
    "                                - trans_agg.withdr * trans_agg.amount_trans\n",
    "    trans_agg['net_bdeposit'] =  trans_agg.b_deposit * trans_agg.amount_trans \\\n",
    "                                - trans_agg.b_withdr * trans_agg.amount_trans\n",
    "\n",
    "    trans_agg = trans_agg.groupby('loan_id').agg({\n",
    "        'balance':['min','mean','max'],\n",
    "        'c_deposit':'sum',\n",
    "        'withdr':'sum',\n",
    "        'sanc':'max',\n",
    "        'balance_start':lambda x: x.iloc[0],\n",
    "        'transactions':'sum',\n",
    "        'net_cdeposit':'sum',\n",
    "        'net_bdeposit':['sum','max']\n",
    "    })\n",
    "    trans_agg.columns = ['_'.join(col).strip() for col in trans_agg.columns.values]\n",
    "\n",
    "    trans_agg['balance_max'] = trans_agg[['balance_max', 'balance_start_<lambda>']].max(axis=1)\n",
    "    trans_agg['balance_min'] = trans_agg[['balance_min', 'balance_start_<lambda>']].min(axis=1)\n",
    "    trans_agg.drop(['balance_start_<lambda>'], axis=1, inplace=True)\n",
    "\n",
    "    return trans_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_agg = aggregate(trans_loans, 100, 0)\n",
    "trans_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge transaction and loan datasets, export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.merge(loans, trans_agg, left_index=True, right_index=True, suffixes=['','_trans'], how='left')\n",
    "\n",
    "\n",
    "\n",
    "with open('loan_data','wb') as file:\n",
    "    pickle.dump(final, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
